# Tech Minds Analytics

**AnÃ¡lise de SaÃºde Mental na IndÃºstria de Tecnologia**

Pipeline completo de ETL e anÃ¡lise de dados sobre saÃºde mental em profissionais de tecnologia, utilizando arquitetura de Data Lake com camadas Bronze, Silver e Gold.

---

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa uma soluÃ§Ã£o completa de CiÃªncia de Dados/Big Data para anÃ¡lise de pesquisa sobre saÃºde mental em trabalhadores da Ã¡rea de tecnologia. O sistema coleta dados brutos, processa, limpa e agrega informaÃ§Ãµes para geraÃ§Ã£o de insights atravÃ©s de dashboards e visualizaÃ§Ãµes.

### Objetivo

Analisar padrÃµes de saÃºde mental na indÃºstria tech, identificando correlaÃ§Ãµes entre fatores como trabalho remoto, idade, gÃªnero, paÃ­s e busca por tratamento psicolÃ³gico.

### Escopo

**IncluÃ­do:**
- Pipeline ETL automatizado (Bronze â†’ Silver â†’ Gold)
- Data Lake em MinIO (compatÃ­vel com S3)
- Processamento de dados com Pandas
- Armazenamento em formatos CSV (raw) e Parquet (processado)
- Dashboard de visualizaÃ§Ã£o com Metabase
- Notebook de anÃ¡lise exploratÃ³ria
- Infraestrutura Docker completa

**NÃ£o IncluÃ­do:**
- Processamento distribuÃ­do (Spark)
- Streaming em tempo real
- Machine Learning / Modelos preditivos
- Pipeline de CI/CD

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Datasets       â”‚ (CSV local)
â”‚  mental_health  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER   â”‚ (IngestÃ£o Bruta)
â”‚   - MinIO       â”‚ raw_mental_health.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ [Limpeza e PadronizaÃ§Ã£o]
         â”‚ â€¢ NormalizaÃ§Ã£o de Gender
         â”‚ â€¢ RemoÃ§Ã£o de outliers (Age)
         â”‚ â€¢ Tratamento de nulos
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER   â”‚ (Dados Limpos)
â”‚   - MinIO       â”‚ mental_health_clean.parquet
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ [AgregaÃ§Ã£o e MÃ©tricas]
         â”‚ â€¢ Group by remote_work + treatment
         â”‚ â€¢ Count by Country
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GOLD LAYER    â”‚ (Dados AnalÃ­ticos)
â”‚   - MinIO       â”‚ agg_remote_work_treatment.parquet
â”‚                 â”‚ agg_country_distribution.parquet
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   METABASE      â”‚ (VisualizaÃ§Ã£o BI)
â”‚  Dashboards     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Camadas do Data Lake

| Camada | DescriÃ§Ã£o | Formato | TransformaÃ§Ãµes |
|--------|-----------|---------|----------------|
| **Bronze** | Dados brutos sem alteraÃ§Ãµes | CSV | Nenhuma |
| **Silver** | Dados limpos e padronizados | Parquet | NormalizaÃ§Ã£o, limpeza, validaÃ§Ã£o |
| **Gold** | Dados agregados para anÃ¡lise | Parquet | AgregaÃ§Ãµes, mÃ©tricas, KPIs |

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.11**: Linguagem principal
- **Apache Airflow 2.8**: OrquestraÃ§Ã£o e agendamento de pipelines
- **Pandas**: Processamento de dados
- **MinIO**: Data Lake (storage S3-compatible)
- **PostgreSQL**: Banco de metadados do Airflow
- **Metabase**: Dashboard e BI
- **Docker**: ContainerizaÃ§Ã£o
- **Jupyter**: AnÃ¡lise exploratÃ³ria
- **Parquet**: Formato colunar eficiente

### DependÃªncias Python

```
pandas==2.1.4
minio==7.2.0
pyarrow==14.0.1
matplotlib==3.8.2
seaborn==0.13.0
jupyter==1.0.0
python-dotenv==1.0.0
```

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.11+ (para execuÃ§Ã£o local do ETL)
- 4GB RAM disponÃ­vel

### Passo 1: Subir a Infraestrutura

```bash
# Entre na pasta infra
cd infra

# Inicie os containers (primeira vez pode demorar ~2-3 minutos)
docker-compose up -d

# Aguarde a inicializaÃ§Ã£o do Airflow
docker-compose logs -f airflow-init

# Verifique os serviÃ§os
docker-compose ps
```

**ServiÃ§os disponÃ­veis:**
- **Airflow UI**: http://localhost:8080 (admin / admin)
- **MinIO Console**: http://localhost:9001 (minioadmin / minioadmin)
- **MinIO API**: http://localhost:9000
- **Metabase**: http://localhost:3000

### Passo 2: Instalar DependÃªncias Python

```bash
# Retorne Ã  raiz do projeto
cd ..

# Crie um ambiente virtual (opcional mas recomendado)
python -m venv venv
.\venv\Scripts\Activate  # Windows
source venv/bin/activate  # Linux/Mac

# Instale as dependÃªncias
pip install -r requirements.txt
```

### Passo 3: Executar o Pipeline ETL

**OpÃ§Ã£o A - Via Airflow (Recomendado):**
```bash
# 1. Acesse a UI do Airflow: http://localhost:8080
# 2. Login: admin / admin
# 3. Ative a DAG "mental_health_etl_pipeline"
# 4. Clique em "Trigger DAG" para executar
```

**OpÃ§Ã£o B - ExecuÃ§Ã£o Manual:**
```bash
# Execute o script ETL diretamente
python src/etl.py
```

O pipeline executa:
1. Criar buckets no MinIO (bronze, silver, gold)
2. Ler `datasets/mental_health.csv`
3. Processar dados atravÃ©s das 3 camadas
4. Gerar agregaÃ§Ãµes finais
5. Validar dados processados

### Passo 4: Explorar os Dados

**OpÃ§Ã£o A - Verificar dados processados:**
```bash
python src/verify_gold_data.py
```

**OpÃ§Ã£o B - Notebook Jupyter:**
```bash
jupyter notebook notebooks/01_exploratory_analysis.ipynb
```

**OpÃ§Ã£o C - Metabase:**
1. Acesse http://localhost:3000
2. Configure conexÃ£o com MinIO
3. Explore os dados das camadas Silver e Gold

---

## ğŸ“ Estrutura do Projeto

```
Projeto_Final/
â”œâ”€â”€ airflow/                     # Apache Airflow
â”‚   â”œâ”€â”€ dags/                    # DAGs do Airflow
â”‚   â”‚   â””â”€â”€ mental_health_etl_dag.py
â”‚   â”œâ”€â”€ logs/                    # Logs de execuÃ§Ã£o
â”‚   â””â”€â”€ plugins/                 # Plugins customizados
â”œâ”€â”€ datasets/                    # Dados brutos
â”‚   â””â”€â”€ mental_health.csv
â”œâ”€â”€ src/                         # CÃ³digo-fonte
â”‚   â”œâ”€â”€ etl.py                   # Pipeline ETL principal
â”‚   â””â”€â”€ verify_gold_data.py      # Script de verificaÃ§Ã£o
â”œâ”€â”€ notebooks/                   # AnÃ¡lises exploratÃ³rias
â”‚   â””â”€â”€ 01_exploratory_analysis.ipynb
â”œâ”€â”€ infra/                       # Infraestrutura
â”‚   â”œâ”€â”€ docker-compose.yml       # OrquestraÃ§Ã£o (Airflow, MinIO, Metabase, PostgreSQL)
â”‚   â””â”€â”€ Dockerfile               # Imagem Python customizada
â”œâ”€â”€ docs/                        # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ architecture.md          # Detalhes da arquitetura
â”‚   â”œâ”€â”€ data_dictionary.md       # DicionÃ¡rio de dados
â”‚   â””â”€â”€ airflow_guide.md         # Guia do Airflow
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ .env.example                 # Exemplo de variÃ¡veis de ambiente
â””â”€â”€ README.md                    # Este arquivo
```

---

## ğŸ“Š DescriÃ§Ã£o dos Dados

### Dataset Original

**Fonte:** Pesquisa sobre saÃºde mental em trabalhadores de tecnologia  
**Formato:** CSV  
**Tamanho:** ~1200 registros, 27 colunas

### Principais Colunas

| Coluna | Tipo | DescriÃ§Ã£o | TransformaÃ§Ãµes |
|--------|------|-----------|----------------|
| `Age` | int | Idade do respondente | Filtro: 18-100 anos, preenchimento de nulos com mediana |
| `Gender` | string | Identidade de gÃªnero | NormalizaÃ§Ã£o: Male/Female/Other |
| `Country` | string | PaÃ­s de residÃªncia | Preenchimento de nulos: "Unknown" |
| `remote_work` | string | Trabalha remotamente (Yes/No) | Preenchimento de nulos: "Unknown" |
| `treatment` | string | Buscou tratamento (Yes/No) | Preenchimento de nulos: "Unknown" |
| `tech_company` | string | Trabalha em empresa tech | Preenchimento de nulos: "Unknown" |

### AgregaÃ§Ãµes Geradas (Gold Layer)

1. **agg_remote_work_treatment.parquet**
   - Cruzamento: trabalho remoto Ã— busca por tratamento
   - Uso: Analisar impacto do trabalho remoto na saÃºde mental

2. **agg_country_distribution.parquet**
   - DistribuiÃ§Ã£o geogrÃ¡fica dos respondentes
   - Uso: Identificar paÃ­ses com maior participaÃ§Ã£o

---

## ğŸ”§ DecisÃµes TÃ©cnicas

### Por que MinIO?

- **Compatibilidade S3**: Facilita migraÃ§Ã£o para AWS/Azure no futuro
- **Open-source**: Sem custos de licenÃ§a
- **Performance**: Otimizado para object storage
- **Docker-friendly**: FÃ¡cil setup local

### Por que Parquet?

- **CompressÃ£o**: ~60% menor que CSV
- **Performance**: Leitura colunar eficiente
- **Schema**: Tipagem forte de dados
- **CompatÃ­vel**: Funciona com Spark, Pandas, DuckDB, etc.

### Por que Metabase?

- **Open-source**: Gratuito e extensÃ­vel
- **Setup rÃ¡pido**: Funciona out-of-the-box
- **SQL-friendly**: Queries diretas nos dados
- **Alternativa considerada**: Grafana (mais focado em mÃ©tricas de infraestrutura)

---

## âš ï¸ LimitaÃ§Ãµes e Pontos de Falha

### LimitaÃ§Ãµes Conhecidas

1. **Escalabilidade**: Pandas nÃ£o Ã© ideal para datasets > 10GB
   - **MitigaÃ§Ã£o futura**: Migrar para PySpark
   
2. **ResiliÃªncia**: ETL falha completamente se um bucket nÃ£o existir
   - **MitigaÃ§Ã£o**: Script cria buckets automaticamente
   
3. **Versionamento**: NÃ£o hÃ¡ controle de versÃ£o dos dados
   - **MitigaÃ§Ã£o futura**: Implementar Delta Lake

4. **Monitoramento**: AusÃªncia de alertas e mÃ©tricas
   - **MitigaÃ§Ã£o futura**: Integrar Prometheus + Grafana

### Pontos de Falha

- **MinIO offline**: ETL falha completamente
- **Arquivo CSV corrompido**: Bronze layer falha
- **Colunas faltantes**: Silver/Gold podem quebrar
- **MemÃ³ria insuficiente**: Pandas pode travar com datasets grandes

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Verificar Dados Processados

```bash
python src/verify_gold_data.py
```

Este script valida:
- âœ… ExistÃªncia dos buckets
- âœ… PresenÃ§a dos arquivos nas camadas
- âœ… Integridade dos dados Parquet
- âœ… Contagem de registros

---

## ğŸ“ˆ Melhorias Futuras

- [ ] Adicionar Apache Airflow para orquestraÃ§Ã£o
- [ ] Implementar testes unitÃ¡rios (pytest)
- [ ] Adicionar Great Expectations para data quality
- [ ] Migrar para PySpark para escalabilidade
- [ ] Implementar Delta Lake para versionamento
- [ ] Adicionar API REST (FastAPI) para servir dados
- [ ] Criar pipeline de ML para prediÃ§Ã£o de risco
- [ ] Implementar CDC (Change Data Capture)

---

## ğŸ‘¥ Equipe

### Responsabilidades Individuais

_[Preencher com os nomes e responsabilidades de cada membro do grupo]_

| Nome | Responsabilidade |
|------|------------------|
| _Membro 1_ | Arquitetura e ETL |
| _Membro 2_ | AnÃ¡lise exploratÃ³ria e notebooks |
| _Membro 3_ | Infraestrutura Docker |
| _Membro 4_ | Dashboards e visualizaÃ§Ãµes |
| _Membro 5_ | DocumentaÃ§Ã£o e testes |

---

## ğŸ“ Suporte

Em caso de dÃºvidas ou problemas:

1. Verifique os logs do Docker: `docker-compose logs -f`
2. Consulte a documentaÃ§Ã£o no diretÃ³rio `docs/`
3. Valide os dados com `verify_gold_data.py`

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© parte de um trabalho acadÃªmico para a disciplina de Big Data e CiÃªncia de Dados.

---

**Ãšltima atualizaÃ§Ã£o:** Novembro 2025
