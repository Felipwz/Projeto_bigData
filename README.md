# Tech Minds Analytics

**An√°lise de Sa√∫de Mental na Ind√∫stria de Tecnologia**

Pipeline completo de ETL e an√°lise de dados sobre sa√∫de mental em profissionais de tecnologia, utilizando arquitetura de Data Lake com camadas Bronze, Silver e Gold.

---

## üìã Vis√£o Geral

Este projeto implementa uma solu√ß√£o completa de Ci√™ncia de Dados/Big Data para an√°lise de pesquisa sobre sa√∫de mental em trabalhadores da √°rea de tecnologia. O sistema coleta dados brutos, processa, limpa e agrega informa√ß√µes para gera√ß√£o de insights atrav√©s de dashboards e visualiza√ß√µes.

### Objetivo

Analisar padr√µes de sa√∫de mental na ind√∫stria tech, identificando correla√ß√µes entre fatores como trabalho remoto, idade, g√™nero, pa√≠s e busca por tratamento psicol√≥gico.

### Escopo

**Inclu√≠do:**
- Pipeline ETL automatizado (Bronze ‚Üí Silver ‚Üí Gold)
- Data Lake em MinIO (compat√≠vel com S3)
- Processamento de dados com Pandas
- Armazenamento em formatos CSV (raw) e Parquet (processado)
- Dashboard de visualiza√ß√£o com Metabase
- Notebook de an√°lise explorat√≥ria
- Infraestrutura Docker completa

**N√£o Inclu√≠do:**
- Processamento distribu√≠do (Spark)
- Streaming em tempo real
- Machine Learning / Modelos preditivos
- Pipeline de CI/CD

---

**Documenta√ß√£o Confluence**

 - https://vitorguimap.atlassian.net/wiki/external/YTNkMWJjODljZDkzNGViZThmZjlmYjRmMGVjMWQyODk

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Datasets       ‚îÇ (CSV local)
‚îÇ  mental_health  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BRONZE LAYER   ‚îÇ (Ingest√£o Bruta)
‚îÇ   - MinIO       ‚îÇ raw_mental_health.csv
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº [Limpeza e Padroniza√ß√£o]
         ‚îÇ ‚Ä¢ Normaliza√ß√£o de Gender
         ‚îÇ ‚Ä¢ Remo√ß√£o de outliers (Age)
         ‚îÇ ‚Ä¢ Tratamento de nulos
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SILVER LAYER   ‚îÇ (Dados Limpos)
‚îÇ   - MinIO       ‚îÇ mental_health_clean.parquet
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº [Agrega√ß√£o e M√©tricas]
         ‚îÇ ‚Ä¢ Group by remote_work + treatment
         ‚îÇ ‚Ä¢ Count by Country
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   GOLD LAYER    ‚îÇ (Dados Anal√≠ticos)
‚îÇ   - MinIO       ‚îÇ agg_remote_work_treatment.parquet
‚îÇ                 ‚îÇ agg_country_distribution.parquet
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   METABASE      ‚îÇ (Visualiza√ß√£o BI)
‚îÇ  Dashboards     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Camadas do Data Lake

| Camada | Descri√ß√£o | Formato | Transforma√ß√µes |
|--------|-----------|---------|----------------|
| **Bronze** | Dados brutos sem altera√ß√µes | CSV | Nenhuma |
| **Silver** | Dados limpos e padronizados | Parquet | Normaliza√ß√£o, limpeza, valida√ß√£o |
| **Gold** | Dados agregados para an√°lise | Parquet | Agrega√ß√µes, m√©tricas, KPIs |

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.11**: Linguagem principal
- **Apache Airflow 2.8**: Orquestra√ß√£o e agendamento de pipelines
- **Pandas**: Processamento de dados
- **MinIO**: Data Lake (storage S3-compatible)
- **PostgreSQL**: Banco de metadados do Airflow
- **Metabase**: Dashboard e BI
- **Docker**: Containeriza√ß√£o
- **Jupyter**: An√°lise explorat√≥ria
- **Parquet**: Formato colunar eficiente

### Depend√™ncias Python

```
pandas==2.1.4
minio==7.2.0
pyarrow==14.0.1
matplotlib==3.8.2
seaborn==0.13.0
jupyter==1.0.0
python-dotenv==1.0.0
```

---

## üöÄ Como Executar

### Pr√©-requisitos

- Docker e Docker Compose instalados
- Python 3.11+ (para execu√ß√£o local do ETL)
- 4GB RAM dispon√≠vel

### Passo 1: Subir a Infraestrutura

```bash
# Entre na pasta infra
cd infra

# Inicie os containers (primeira vez pode demorar ~2-3 minutos)
docker-compose up -d

# Aguarde a inicializa√ß√£o do Airflow
docker-compose logs -f airflow-init

# Verifique os servi√ßos
docker-compose ps
```
**No Metabase:**

1. Clique na **engrengem (Admin)** ‚Üí **Databases**  
2. Clique em **+ Add database**  
3. Selecione **PostgreSQL**  
4. Preencha os campos conforme abaixo:

   - **Database name:** `airflow`
   - **Display name:** `Airflow DB (PostgreSQL)`
   - **Host:** `postgres`
   - **Port:** `5432`
   - **Username:** `airflow`
   - **Password:** `airflow`

5. Clique em **Save**

### Configura√ß√£o do Data Lake (MinIO) no Metabase

Para visualizar os dados das camadas Silver e Gold, √© preciso conectar o Metabase ao MinIO. Como o Metabase n√£o possui um conector nativo para MinIO/S3, a abordagem recomendada neste projeto √© carregar os dados agregados (camada Gold) no PostgreSQL para facilitar a visualiza√ß√£o.

1.  **Carregar Dados no PostgreSQL:**
    O projeto inclui um script para carregar os dados do MinIO para o PostgreSQL. Execute o seguinte comando no terminal, na raiz do projeto, para popular as tabelas que o Metabase ir√° ler.

    ```bash
    # O comando abaixo executa o script que l√™ os arquivos .parquet do MinIO 
    # e os insere como tabelas no banco 'airflow' do PostgreSQL.
    docker-compose -f infra/docker-compose.yml exec airflow-scheduler python /opt/airflow/src/load_to_postgres.py
    ```

2.  **Explorar no Metabase:**
    Ap√≥s executar o script, novas tabelas (ex: `gold_remote_work_treatment`) estar√£o dispon√≠veis no database `Airflow DB (PostgreSQL)` dentro do Metabase, prontas para serem usadas em perguntas e dashboards.

**Exemplos de consulta para o dashboard**

```
Camada Bronze

SELECT * FROM bronze_layer LIMIT 10;
SELECT COUNT(*) FROM bronze_layer;
SELECT DISTINCT gender, COUNT(*) FROM bronze_layer GROUP BY gender;
```
```
Camada Silver

SELECT * FROM silver_layer LIMIT 10;
SELECT gender, COUNT(*) FROM silver_layer GROUP BY gender;
SELECT remote_work, treatment, COUNT(*) FROM silver_layer GROUP BY remote_work, treatment;

```
Camada Gold

SELECT * FROM gold_remote_work_treatment;
SELECT * FROM gold_country_distribution ORDER BY count DESC LIMIT 10;
```
gittgt
-- An√°lise por faixa et√°ria e tratamento
SELECT
    CASE
        WHEN age BETWEEN 18 AND 25 THEN '18-25'
        WHEN age BETWEEN 26 AND 35 THEN '26-35'
        WHEN age BETWEEN 36 AND 45 THEN '36-45'
        WHEN age BETWEEN 46 AND 60 THEN '46-60'
        ELSE '60+'
    END as faixa_etaria,
    treatment,
    COUNT(*) as quantidade
FROM silver_layer
GROUP BY faixa_etaria, treatment
ORDER BY faixa_etaria, treatment;

Comando para alimentar Postgres com os dados do minio

Alterar de acordo com o computador o path
cd /home/guima/Documents/Projeto_bigData/infra && docker-compose exec -T airflow-scheduler python /opt/airflow/src/load_to_postgres.py 2>&1 | tail -30

**Servi√ßos dispon√≠veis:**
- **Airflow UI**: http://localhost:8080 (admin / admin)
- **MinIO Console**: http://localhost:9001 (minioadmin / minioadmin)
- **MinIO API**: http://localhost:9000
- **Metabase**: http://localhost:3000

### Passo 2: Instalar Depend√™ncias Python

```bash
# Retorne √† raiz do projeto
cd ..

# Crie um ambiente virtual (opcional mas recomendado)
python -m venv venv
.\venv\Scripts\Activate  # Windows
source venv/bin/activate  # Linux/Mac

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Passo 3: Executar o Pipeline ETL

**Op√ß√£o A - Via Airflow (Recomendado):**
```bash
# 1. Acesse a UI do Airflow: http://localhost:8080
# 2. Login: admin / admin
# 3. Ative a DAG "mental_health_etl_pipeline"
# 4. Clique em "Trigger DAG" para executar
```

**Op√ß√£o B - Execu√ß√£o Manual:**
```bash
# Execute o script ETL diretamente
python src/etl.py
```

O pipeline executa:
1. Criar buckets no MinIO (bronze, silver, gold)
2. Ler `datasets/mental_health.csv`
3. Processar dados atrav√©s das 3 camadas
4. Gerar agrega√ß√µes finais
5. Validar dados processados

### Passo 4: Explorar os Dados

**Op√ß√£o A - Verificar dados processados:**
```bash
python src/verify_gold_data.py
```

**Op√ß√£o B - Notebook Jupyter:**
```bash
jupyter notebook notebooks/01_exploratory_analysis.ipynb
```

**Op√ß√£o C - Metabase:**
1. Acesse http://localhost:3000
2. Configure conex√£o com MinIO
3. Explore os dados das camadas Silver e Gold

---

## üìÅ Estrutura do Projeto

```
Projeto_Final/
‚îú‚îÄ‚îÄ airflow/                     # Apache Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dags/                    # DAGs do Airflow
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mental_health_etl_dag.py
‚îÇ   ‚îú‚îÄ‚îÄ logs/                    # Logs de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ plugins/                 # Plugins customizados
‚îú‚îÄ‚îÄ datasets/                    # Dados brutos
‚îÇ   ‚îî‚îÄ‚îÄ mental_health.csv
‚îú‚îÄ‚îÄ src/                         # C√≥digo-fonte
‚îÇ   ‚îú‚îÄ‚îÄ etl.py                   # Pipeline ETL principal
‚îÇ   ‚îî‚îÄ‚îÄ verify_gold_data.py      # Script de verifica√ß√£o
‚îú‚îÄ‚îÄ notebooks/                   # An√°lises explorat√≥rias
‚îÇ   ‚îî‚îÄ‚îÄ 01_exploratory_analysis.ipynb
‚îú‚îÄ‚îÄ infra/                       # Infraestrutura
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml       # Orquestra√ß√£o (Airflow, MinIO, Metabase, PostgreSQL)
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile               # Imagem Python customizada
‚îú‚îÄ‚îÄ docs/                        # Documenta√ß√£o t√©cnica
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md          # Detalhes da arquitetura
‚îÇ   ‚îú‚îÄ‚îÄ data_dictionary.md       # Dicion√°rio de dados
‚îÇ   ‚îî‚îÄ‚îÄ airflow_guide.md         # Guia do Airflow
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example                 # Exemplo de vari√°veis de ambiente
‚îî‚îÄ‚îÄ README.md                    # Este arquivo
```

---

## üìä Descri√ß√£o dos Dados

### Dataset Original

**Fonte:** Pesquisa sobre sa√∫de mental em trabalhadores de tecnologia  
**Formato:** CSV  
**Tamanho:** ~1200 registros, 27 colunas

### Principais Colunas

| Coluna | Tipo | Descri√ß√£o | Transforma√ß√µes |
|--------|------|-----------|----------------|
| `Age` | int | Idade do respondente | Filtro: 18-100 anos, preenchimento de nulos com mediana |
| `Gender` | string | Identidade de g√™nero | Normaliza√ß√£o: Male/Female/Other |
| `Country` | string | Pa√≠s de resid√™ncia | Preenchimento de nulos: "Unknown" |
| `remote_work` | string | Trabalha remotamente (Yes/No) | Preenchimento de nulos: "Unknown" |
| `treatment` | string | Buscou tratamento (Yes/No) | Preenchimento de nulos: "Unknown" |
| `tech_company` | string | Trabalha em empresa tech | Preenchimento de nulos: "Unknown" |

### Agrega√ß√µes Geradas (Gold Layer)

1. **agg_remote_work_treatment.parquet**
   - Cruzamento: trabalho remoto √ó busca por tratamento
   - Uso: Analisar impacto do trabalho remoto na sa√∫de mental

2. **agg_country_distribution.parquet**
   - Distribui√ß√£o geogr√°fica dos respondentes
   - Uso: Identificar pa√≠ses com maior participa√ß√£o

---

## üîß Decis√µes T√©cnicas

### Por que MinIO?

- **Compatibilidade S3**: Facilita migra√ß√£o para AWS/Azure no futuro
- **Open-source**: Sem custos de licen√ßa
- **Performance**: Otimizado para object storage
- **Docker-friendly**: F√°cil setup local

### Por que Parquet?

- **Compress√£o**: ~60% menor que CSV
- **Performance**: Leitura colunar eficiente
- **Schema**: Tipagem forte de dados
- **Compat√≠vel**: Funciona com Spark, Pandas, DuckDB, etc.

### Por que Metabase?

- **Open-source**: Gratuito e extens√≠vel
- **Setup r√°pido**: Funciona out-of-the-box
- **SQL-friendly**: Queries diretas nos dados
- **Alternativa considerada**: Grafana (mais focado em m√©tricas de infraestrutura)

---

## ‚ö†Ô∏è Limita√ß√µes e Pontos de Falha

### Limita√ß√µes Conhecidas

1. **Escalabilidade**: Pandas n√£o √© ideal para datasets > 10GB
   - **Mitiga√ß√£o futura**: Migrar para PySpark
   
2. **Resili√™ncia**: ETL falha completamente se um bucket n√£o existir
   - **Mitiga√ß√£o**: Script cria buckets automaticamente
   
3. **Versionamento**: N√£o h√° controle de vers√£o dos dados
   - **Mitiga√ß√£o futura**: Implementar Delta Lake

4. **Monitoramento**: Aus√™ncia de alertas e m√©tricas
   - **Mitiga√ß√£o futura**: Integrar Prometheus + Grafana

### Pontos de Falha

- **MinIO offline**: ETL falha completamente
- **Arquivo CSV corrompido**: Bronze layer falha
- **Colunas faltantes**: Silver/Gold podem quebrar
- **Mem√≥ria insuficiente**: Pandas pode travar com datasets grandes

---

## üß™ Testes e Valida√ß√£o

### Verificar Dados Processados

```bash
python src/verify_gold_data.py
```

Este script valida:
- ‚úÖ Exist√™ncia dos buckets
- ‚úÖ Presen√ßa dos arquivos nas camadas
- ‚úÖ Integridade dos dados Parquet
- ‚úÖ Contagem de registros

---

## üìà Melhorias Futuras

- [ ] Adicionar Apache Airflow para orquestra√ß√£o
- [ ] Refinar a orquestra√ß√£o com Airflow (ex: adicionar alertas em caso de falha, usar XComs para passar metadados entre tarefas)
- [ ] Implementar testes unit√°rios (pytest)
- [ ] Adicionar Great Expectations para data quality
- [ ] Migrar para PySpark para escalabilidade
- [ ] Implementar Delta Lake para versionamento
- [ ] Adicionar API REST (FastAPI) para servir dados
- [ ] Criar pipeline de ML para predi√ß√£o de risco
- [ ] Implementar CDC (Change Data Capture)

---

## üë• Equipe

### Responsabilidades Individuais

Cada membro do grupo √© respons√°vel por explicar sua √°rea espec√≠fica durante a apresenta√ß√£o:

| Membro | √Årea de Responsabilidade | Componentes | O que Explicar na Apresenta√ß√£o |
|--------|--------------------------|-------------|--------------------------------|
| **Natan** | **Arquitetura & Pipeline ETL** | ‚Ä¢ Data Lake (MinIO)<br>‚Ä¢ Pipeline Bronze‚ÜíSilver‚ÜíGold<br>‚Ä¢ `src/etl.py` | ‚Ä¢ Como funciona a arquitetura Medallion<br>‚Ä¢ Transforma√ß√µes em cada camada<br>‚Ä¢ Por que MinIO e Parquet<br>‚Ä¢ Fluxo de dados completo |
| **Leonardo** | **Orquestra√ß√£o & Automa√ß√£o** | ‚Ä¢ Apache Airflow<br>‚Ä¢ DAG (`mental_health_etl_dag.py`)<br>‚Ä¢ Agendamento | ‚Ä¢ Como o Airflow orquestra o pipeline<br>‚Ä¢ Tasks e depend√™ncias<br>‚Ä¢ Agendamento @daily<br>‚Ä¢ Monitoramento e logs |
| **Vitor** | **Infraestrutura & DevOps** | ‚Ä¢ Docker Compose<br>‚Ä¢ PostgreSQL<br>‚Ä¢ Metabase<br>‚Ä¢ Configura√ß√µes | ‚Ä¢ Como a infraestrutura funciona<br>‚Ä¢ Servi√ßos Docker (6 containers)<br>‚Ä¢ Como rodar o projeto do zero<br>‚Ä¢ Troubleshooting |
| **Luiz Felipe** | **An√°lise de Dados & Documenta√ß√£o** | ‚Ä¢ Jupyter Notebook<br>‚Ä¢ Dicion√°rio de dados<br>‚Ä¢ Documenta√ß√£o t√©cnica<br>‚Ä¢ Valida√ß√£o | ‚Ä¢ Insights da an√°lise explorat√≥ria<br>‚Ä¢ Qualidade dos dados<br>‚Ä¢ Visualiza√ß√µes e KPIs<br>‚Ä¢ Estrutura da documenta√ß√£o |

---

## üìû Suporte

Em caso de d√∫vidas ou problemas:

1. Verifique os logs do Docker: `docker-compose logs -f`
2. Consulte a documenta√ß√£o no diret√≥rio `docs/`
3. Valide os dados com `verify_gold_data.py`

---

##  Equipe

- Luiz Felipe S. de Souza (6324548)
- Leonardo Fraz√£o Sano (6324073)
- Natan Borges Leme (6324696)
- Vitor Pinheiro Guimar√£es (6324680)


---

**√öltima atualiza√ß√£o:** Dezembro 2025
